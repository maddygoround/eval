# AI Evaluator MCP Server - Project Overview

## üìÅ Project Structure

```
ai-evaluator-mcp/
‚îú‚îÄ‚îÄ server.py              # Main MCP server (entry point)
‚îú‚îÄ‚îÄ evaluator.py           # Core evaluation engine using Inspect AI
‚îú‚îÄ‚îÄ judge.py               # Petri-style multi-dimensional judge
‚îú‚îÄ‚îÄ storage.py             # SQLite persistence layer
‚îú‚îÄ‚îÄ example_client.py      # Usage examples and client wrapper
‚îú‚îÄ‚îÄ test_evaluator.py      # Test suite
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ setup.py              # Package installation config
‚îú‚îÄ‚îÄ .env.example          # Environment configuration template
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ README.md            # Full documentation
‚îî‚îÄ‚îÄ QUICKSTART.md        # Quick start guide
```

## üéØ Core Components

### 1. MCP Server (`server.py`)
- **Purpose**: Exposes evaluation tools via MCP protocol
- **Tools Provided**:
  - `evaluate_response` - Comprehensive evaluation
  - `check_hallucinations` - Quick hallucination scan
  - `verify_tool_consistency` - Tool usage verification
  - `compare_model_responses` - Multi-model comparison
  - `get_session_report` - Session statistics
  - `start_evaluation_session` - Session management

### 2. Evaluator (`evaluator.py`)
- **Framework**: Built on Inspect AI
- **Capabilities**:
  - Pattern-based detection (regex for common issues)
  - Model-graded evaluation (uses judge model)
  - Multi-dimensional scoring
  - Comprehensive analysis pipeline

**Evaluation Pipeline**:
```
1. Pattern Matching ‚Üí Quick detection of obvious issues
2. Judge Model Eval ‚Üí Deep analysis using LLM
3. Scoring ‚Üí Aggregate scores across dimensions
4. Recommendations ‚Üí Actionable suggestions
```

### 3. Petri Judge (`judge.py`)
- **Inspiration**: Anthropic's Petri behavioral evaluation framework
- **Dimensions** (6 total):
  1. **Truthfulness** - Verifiable, accurate claims
  2. **Tool Reliability** - Honest tool usage reporting
  3. **Consistency** - No contradictions
  4. **Appropriateness** - Relevant to query
  5. **Safety** - Avoids harmful content
  6. **Calibration** - Appropriate confidence

**Judge Pattern** (from Petri):
```
Detailed Prompt ‚Üí Judge Model ‚Üí Structured Evaluation ‚Üí Aggregate Score
```

### 4. Storage (`storage.py`)
- **Database**: SQLite (local, file-based)
- **Tables**:
  - `sessions` - Evaluation sessions
  - `evaluations` - Individual response evaluations
  - `issues` - Detailed issue tracking
- **Features**: Session stats, issue tracking, export to JSON

## üîÑ How It Works

### Evaluation Flow

```
User's AI Response
      ‚Üì
  MCP Server (receives evaluation request)
      ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ         Evaluator Pipeline          ‚îÇ
  ‚îÇ                                     ‚îÇ
  ‚îÇ  1. Pattern Detection               ‚îÇ
  ‚îÇ     - Regex for hallucinations     ‚îÇ
  ‚îÇ     - Tool claim patterns          ‚îÇ
  ‚îÇ     - Overconfidence markers       ‚îÇ
  ‚îÇ                                     ‚îÇ
  ‚îÇ  2. Judge Evaluation (Petri)       ‚îÇ
  ‚îÇ     - Multi-dimensional scoring    ‚îÇ
  ‚îÇ     - Evidence-based assessment    ‚îÇ
  ‚îÇ     - Structured feedback          ‚îÇ
  ‚îÇ                                     ‚îÇ
  ‚îÇ  3. Aggregation                    ‚îÇ
  ‚îÇ     - Combine all scores           ‚îÇ
  ‚îÇ     - Generate warnings            ‚îÇ
  ‚îÇ     - Create suggestions           ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚Üì
  Storage (save results)
      ‚Üì
  Return to User
  {
    "score": 0.75,
    "warnings": [...],
    "suggestions": [...]
  }
```

### Integration with Inspect AI

**Inspect AI provides**:
- Evaluation task framework
- Model-graded scoring
- Dataset handling
- Standard evaluation patterns

**We use it for**:
- Hallucination detection tasks
- Consistency checking tasks
- Judge model integration
- Reproducible evaluations

### Integration with Petri Patterns

**Petri's approach**:
- Multi-agent system (Auditor ‚Üí Target ‚Üí Judge)
- Multi-dimensional scoring
- Evidence-based evaluation
- Behavioral assessment

**We adapted it for**:
- Development-time evaluation
- Real-time feedback
- Quality assurance
- Model comparison

## üõ†Ô∏è Technical Details

### Why Python?

Both Inspect AI and Petri are Python frameworks. TypeScript integration would require:
- Complex FFI/subprocess communication
- Duplicated evaluation logic
- Increased latency
- Maintenance burden

Python provides:
- Native Inspect AI integration
- Direct Anthropic API access
- Simpler architecture
- Better performance

### Judge Model Selection

**Default: Claude Opus 4.1**
- Highest quality evaluations
- Most accurate hallucination detection
- Best for critical applications

**Alternative: Claude Sonnet 4.5**
- Faster evaluations
- Lower cost
- Good for development testing

### Performance Considerations

**Latency**:
- Pattern matching: ~10ms
- Judge evaluation: ~2-5s (depends on response complexity)
- Total: ~2-5s per evaluation

**Optimization strategies**:
1. Cache repeated evaluations
2. Parallel processing for batch evaluations
3. Async throughout
4. Pattern matching pre-filter (skip judge if obvious pass/fail)

### Accuracy

**Pattern Matching**:
- Fast but limited
- ~70% precision
- Few false negatives (catches most issues)

**Judge Model**:
- Slower but comprehensive
- ~90% precision (with Opus)
- Nuanced understanding

**Combined**:
- Best of both worlds
- High coverage, high precision
- Actionable feedback

## üéì Design Decisions

### 1. Why MCP?
- Standard protocol
- Easy integration with Claude Desktop
- Tool-based interface
- Familiar to developers

### 2. Why SQLite?
- Simple, file-based
- No server setup required
- Good for development use
- Easy to export

### 3. Why Multi-Dimensional Scoring?
- More informative than single score
- Identifies specific issues
- Guides improvements
- Matches Petri's approach

### 4. Why Session Tracking?
- Compare across time
- Track improvements
- A/B testing support
- CI/CD integration

## üöÄ Use Cases

### During Development
```python
# Test your agent
response = agent.process(input)
eval = await evaluator.evaluate(response)
if not eval["pass"]:
    print(f"Fix needed: {eval['suggestions']}")
```

### In Tests
```python
@pytest.mark.asyncio
async def test_quality():
    result = await evaluator.evaluate(response)
    assert result["score"] >= 0.7
```

### In CI/CD
```bash
# Quality gate
python evaluate_deployment.py || exit 1
```

### For Model Selection
```python
# Compare models
results = await evaluator.compare_models([
    {"model": "gpt-4", "response": r1},
    {"model": "claude", "response": r2}
])
print(f"Best: {results['best_model']}")
```

## üìä Metrics & Thresholds

### Score Interpretation
- **0.9-1.0**: Excellent - Production ready
- **0.7-0.9**: Good - Minor improvements possible
- **0.5-0.7**: Fair - Needs attention
- **0.0-0.5**: Poor - Should not deploy

### Default Thresholds (configurable)
- Hallucination tolerance: 0.6
- Tool consistency: 0.7
- Context consistency: 0.7
- Overall pass: 0.7

## üîÆ Future Enhancements

### Potential Additions
1. **Custom domains** - Fine-tune for specific industries
2. **More judge models** - Support local models
3. **Web UI** - Dashboard for evaluation results
4. **Real-time monitoring** - Stream evaluations
5. **Advanced caching** - Redis integration
6. **Model organisms** - Test with intentionally flawed models

### Integration Opportunities
1. **LangChain** - Evaluate LangChain agents
2. **LlamaIndex** - RAG quality assessment
3. **AutoGPT** - Autonomous agent evaluation
4. **Custom frameworks** - Plugin architecture

## ü§ù Contributing

Key areas for contribution:
- New evaluation patterns
- Additional Petri dimensions
- Performance optimizations
- Integration examples
- Documentation improvements

## üìù License

MIT - Use freely for development and production

## üôè Acknowledgments

Built with:
- [Inspect AI](https://github.com/UKGovernmentBEIS/inspect_ai) by UK AISI
- [Petri](https://github.com/anthropics/petri) by Anthropic
- [MCP](https://modelcontextprotocol.io/) by Anthropic

---

**This is a development-first tool. Use it to catch AI quality issues before your users do.**
